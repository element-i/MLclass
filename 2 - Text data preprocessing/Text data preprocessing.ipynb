{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-the-data\" data-toc-modified-id=\"1.-Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Load the data</a></span></li><li><span><a href=\"#2.-Filtering-out-the-noise\" data-toc-modified-id=\"2.-Filtering-out-the-noise-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Filtering out the noise</a></span></li><li><span><a href=\"#3.-Even-better-filtering\" data-toc-modified-id=\"3.-Even-better-filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Even better filtering</a></span></li><li><span><a href=\"#4.-Term-frequency-times-inverse-document-frequency\" data-toc-modified-id=\"4.-Term-frequency-times-inverse-document-frequency-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Term frequency times inverse document frequency</a></span></li><li><span><a href=\"#5.-Utility-function\" data-toc-modified-id=\"5.-Utility-function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Utility function</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Text data pre-processing</div>\n",
    "\n",
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data\n",
    "\n",
    "Let's first load the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:31:00.935088Z",
     "start_time": "2018-10-09T14:31:00.852980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part1/3-425msg1.txt\n",
      "email is a spam: False\n",
      "Subject: what language is this ?\n",
      "\n",
      "the toronto police have contacted our department for help in identifying the language of the label on a ball of wool in the purse of an elderly woman accused of shoplifting . she does not speak english and the police wish to obtain an interpreter for her . the following was dictate to me over the telephone ( so may not be 100 % accurate ) : ata lucru de myna din bumbac cardat . . . . please send replies directly to me . there is some urgency in this , as the woman is being held until they can question her . ron smyth smyth @ lake . scar . utoronto . ca\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_switch=1\n",
    "if(data_switch==0):\n",
    "    train_dir = '../data/ling-spam/train-mails/'\n",
    "    email_path = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "else:\n",
    "    train_dir = '../data/lingspam_public/bare/'\n",
    "    email_path = []\n",
    "    email_label = []\n",
    "    for d in os.listdir(train_dir):\n",
    "        folder = os.path.join(train_dir,d)\n",
    "        email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "        email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 8 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:33:10.652989Z",
     "start_time": "2018-10-09T14:33:06.022590Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:33:20.250716Z",
     "start_time": "2018-10-09T14:33:20.188615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['notes', 'noteworthy', 'nothanku', 'nothofer', 'noti', 'notice', 'noticeably', 'noticed', 'notices', 'noticias', 'noticing', 'notification', 'notifications', 'notificazione', 'notified', 'notifiee', 'notify', 'notifying', 'noting', 'notion', 'notional', 'notionally', 'notionnelle', 'notions', 'notker', 'notorious', 'notoriously', 'notpossible', 'notre', 'notrequire', 'notting', 'nottingham', 'notturno', 'notwendig', 'notwithstanding', 'noufal', 'noun', 'nouniness', 'nouns', 'nous', 'nouvelle', 'nov', 'nova', 'noveau', 'noveck', 'novel', 'novelist', 'novelists', 'novell', 'novell1', 'novels', 'novelty', 'november', 'november1998', 'novembre', 'novenera', 'novetats', 'novi', 'novice', 'novices', 'novick', 'novicklr', 'noviembre', 'novmember', 'novokuznetsk', 'novum', 'novus', 'nowaday', 'nowadays', 'nowak', 'nowens', 'nowflake', 'nowo', 'nowotka', 'noxious', 'noyau', 'noyer', 'nozue', 'np', 'np1', 'nphil', 'npi', 'npis', 'npn', 'nposs', 'npr', 'nps', 'nq', 'nqs', 'nr', 'nrad', 'nrc', 'nrcgsh', 'nreid', 'nrh', 'nrhm', 'nria', 'nrl', 'ns', 'nscee']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[40000:40100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:36:28.608133Z",
     "start_time": "2018-10-09T14:36:22.480670Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "word_count = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:36:28.629323Z",
     "start_time": "2018-10-09T14:36:28.609702Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 14279\n",
      "Document - words matrix: (2893, 14279)\n",
      "First words: ['aa', 'aal', 'aba', 'aback', 'abacus', 'abandon', 'abandoned', 'abandonment', 'abbas', 'abbreviation', 'abdomen', 'abduction', 'abed', 'aberrant', 'aberration', 'abide', 'abiding', 'abigail', 'ability', 'ablative', 'ablaut', 'able', 'abler', 'aboard', 'abolition', 'abord', 'aboriginal', 'aborigine', 'abound', 'abox', 'abreast', 'abridged', 'abroad', 'abrogate', 'abrook', 'abruptly', 'abscissa', 'absence', 'absent', 'absolute', 'absolutely', 'absoluteness', 'absolutist', 'absolutive', 'absolutization', 'absorbed', 'absorption', 'abstract', 'abstraction', 'abstractly', 'abstractness', 'absurd', 'absurdity', 'abu', 'abundance', 'abundant', 'abuse', 'abusive', 'abyss', 'academe', 'academic', 'academically', 'academician', 'academy', 'accelerate', 'accelerated', 'accelerative', 'accent', 'accentuate', 'accentuation', 'accept', 'acceptability', 'acceptable', 'acceptance', 'acceptation', 'accepted', 'acception', 'access', 'accessibility', 'accessible', 'accessibly', 'accidence', 'accident', 'accidental', 'accidentality', 'accidentally', 'acclaim', 'accommodate', 'accommodation', 'accompany', 'accomplish', 'accomplished', 'accomplishment', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountability', 'accountant']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:37:12.633846Z",
     "start_time": "2018-10-09T14:37:12.618441Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: re : 5 . 1196 corpus analysis of - body / - one\n",
      "\n",
      "it seems to me altogether possible , even likely , that there is the following interaction , namely , that the particular lexical form < everybody > has come to acquire , for many speakers certainly ( me , for instance , a quasi native speaker , and i share ellen prince 's intuition concerning her sentence # 1 ) a distinctly collective sense , whereas < every - one > maintains , again for these speakers at least , a distributive sense , at least as a living option . thus 1 ' . everybody came , bringing their respective wives . seems especially good , though , oddly enough maybe , this instantiates the special claim that , for this particular lexical - body item , the formal / informal register distinction has at least contextually ? ) collapsed . now , it seems plausible , but no more than that in the present state of knowledge ( or of ignorance ) that there is a more underlying interaction , namely , that there may be something more naturally colloquial about using a collective rather than a distributed plural when an option presents itself . actually , this follows from an algebraic , non - fregean view of quantification , which i have written on but cannot go into ion this message , save to indicate summarily that it is a capital mistakme to imagine that , for instance , the same semantical reopresentation ought to apply , even for logico-deductive applications , to say , < each / every > and < all > , respectively . thus , invoking a correspndence that i shall not unpack here , i . everybody brought their wives / * everybody brought his wife i ' . they all brought their ( respective ) wives / each man brought his wife . ii . * each man brought their wives / they each brought their ( respective ) wives . this surely suggests , does it not , that the interaction under review is distinctly asymmetrical ( or , i think , rather , non-symmetrical ) , for , the < each > form of quantification , by logical entailment of a fairly obvious kind ( from the distributed , that is , serial , meaning , to the collective / mass , can take a collective sense , whereas the reverse ( you cannot non-arbitrarily unpack a mass b ecause there is no algorithm in this direction correspnding to the seial addition inherent in the other direction ) is not available . all , then , ( and the other collective-sense quantifiers and the nominals that condense such quantifiers ( as in the body cases ) quantiify , i . e . , select as a choice function , over a power set , choosing the element equivalent to the improper 's ubset of the whole ' . each - type quantification is over the elements of a set , taking the index ' i ' ranging , algebraically , successively from 1 ( or , in some cases , 0 ) to n , the cardinality of the set in question . f . k . l . chit hlaing\n",
      "\n",
      "Bag of words representation (134 words in dict):\n",
      "{'subject': array(1), 'corpus': array(1), 'analysis': array(1), 'body': array(3), 'one': array(2), 'altogether': array(1), 'possible': array(1), 'even': array(2), 'likely': array(1), 'following': array(1), 'interaction': array(3), 'namely': array(2), 'particular': array(2), 'lexical': array(2), 'form': array(2), 'everybody': array(4), 'come': array(1), 'acquire': array(1), 'many': array(1), 'certainly': array(1), 'instance': array(2), 'quasi': array(1), 'native': array(1), 'speaker': array(1), 'share': array(1), 'prince': array(1), 'intuition': array(1), 'concerning': array(1), 'sentence': array(1), 'distinctly': array(2), 'collective': array(5), 'sense': array(4), 'whereas': array(2), 'every': array(2), 'least': array(3), 'distributive': array(1), 'living': array(1), 'option': array(2), 'thus': array(2), 'came': array(1), 'respective': array(3), 'especially': array(1), 'good': array(1), 'though': array(1), 'oddly': array(1), 'enough': array(1), 'maybe': array(1), 'special': array(1), 'claim': array(1), 'item': array(1), 'formal': array(1), 'informal': array(1), 'register': array(1), 'distinction': array(1), 'contextually': array(1), 'plausible': array(1), 'present': array(1), 'state': array(1), 'knowledge': array(1), 'ignorance': array(1), 'underlying': array(1), 'may': array(1), 'something': array(1), 'naturally': array(1), 'colloquial': array(1), 'rather': array(2), 'distributed': array(2), 'plural': array(1), 'actually': array(1), 'algebraic': array(1), 'non': array(3), 'view': array(1), 'quantification': array(3), 'written': array(1), 'cannot': array(2), 'go': array(1), 'ion': array(1), 'message': array(1), 'save': array(1), 'indicate': array(1), 'summarily': array(1), 'capital': array(1), 'imagine': array(1), 'semantical': array(1), 'ought': array(1), 'apply': array(1), 'deductive': array(1), 'say': array(1), 'respectively': array(1), 'shall': array(1), 'unpack': array(2), 'brought': array(6), 'wife': array(2), 'man': array(2), 'surely': array(1), 'review': array(1), 'asymmetrical': array(1), 'think': array(1), 'symmetrical': array(1), 'logical': array(1), 'entailment': array(1), 'fairly': array(1), 'obvious': array(1), 'kind': array(1), 'serial': array(1), 'meaning': array(1), 'mass': array(2), 'take': array(1), 'reverse': array(1), 'arbitrarily': array(1), 'algorithm': array(1), 'direction': array(2), 'addition': array(1), 'inherent': array(1), 'available': array(1), 'condense': array(1), 'select': array(1), 'choice': array(1), 'function': array(1), 'power': array(1), 'set': array(3), 'choosing': array(1), 'element': array(1), 'equivalent': array(1), 'improper': array(1), 'whole': array(1), 'type': array(1), 'taking': array(1), 'index': array(1), 'ranging': array(1), 'algebraically': array(1), 'successively': array(1), 'question': array(1), 'chit': array(1)}\n",
      "\n",
      "Vector reprensentation (134 non-zero elements):\n",
      "  (0, 12153)\t1\n",
      "  (0, 2810)\t1\n",
      "  (0, 507)\t1\n",
      "  (0, 1431)\t3\n",
      "  (0, 8596)\t2\n",
      "  (0, 442)\t1\n",
      "  (0, 9496)\t1\n",
      "  (0, 4349)\t2\n",
      "  (0, 7226)\t1\n",
      "  (0, 4923)\t1\n",
      "  (0, 6545)\t3\n",
      "  (0, 8177)\t2\n",
      "  (0, 8922)\t2\n",
      "  (0, 7170)\t2\n",
      "  (0, 4979)\t2\n",
      "  (0, 4358)\t4\n",
      "  (0, 2339)\t1\n",
      "  (0, 127)\t1\n",
      "  (0, 7561)\t1\n",
      "  (0, 1947)\t1\n",
      "  (0, 6476)\t2\n",
      "  (0, 10034)\t1\n",
      "  (0, 8211)\t1\n",
      "  (0, 11779)\t1\n",
      "  (0, 11328)\t1\n",
      "  :\t:\n",
      "  (0, 703)\t1\n",
      "  (0, 375)\t1\n",
      "  (0, 3523)\t2\n",
      "  (0, 169)\t1\n",
      "  (0, 6409)\t1\n",
      "  (0, 972)\t1\n",
      "  (0, 2531)\t1\n",
      "  (0, 11189)\t1\n",
      "  (0, 2077)\t1\n",
      "  (0, 5131)\t1\n",
      "  (0, 9543)\t1\n",
      "  (0, 11276)\t3\n",
      "  (0, 2081)\t1\n",
      "  (0, 4018)\t1\n",
      "  (0, 4262)\t1\n",
      "  (0, 6194)\t1\n",
      "  (0, 14058)\t1\n",
      "  (0, 13172)\t1\n",
      "  (0, 12504)\t1\n",
      "  (0, 6299)\t1\n",
      "  (0, 10125)\t1\n",
      "  (0, 373)\t1\n",
      "  (0, 12224)\t1\n",
      "  (0, 10043)\t1\n",
      "  (0, 2071)\t1\n"
     ]
    }
   ],
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "#print(LemmaTokenizer()(text))\n",
    "#print(len(set(LemmaTokenizer()(text))))\n",
    "#print(len([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(len([word_count2[mail_number, i] for i in word_count2[mail_number, :].nonzero()[1]]))\n",
    "#print(set([feat2word[i] for i in word_count2[mail_number, :].nonzero()[1]])-set(LemmaTokenizer()(text)))\n",
    "emailBagOfWords = {feat2word[i]: word_count[mail_number, i] for i in word_count[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", word_count[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(word_count[mail_number, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:40:42.964414Z",
     "start_time": "2018-10-09T14:40:42.930818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2893, 14279)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(word_count)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every email in the corpus has a vector representation that filters out unrelevant tokens and retains the significant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:40:49.415428Z",
     "start_time": "2018-10-09T14:40:49.408262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email 0:\n",
      "  (0, 14201)\t0.03794448933062206\n",
      "  (0, 14077)\t0.11995441175512732\n",
      "  (0, 14058)\t0.043946205166242404\n",
      "  (0, 14027)\t0.11149263446882586\n",
      "  (0, 13758)\t0.03977564480489188\n",
      "  (0, 13460)\t0.18525919282977144\n",
      "  (0, 13301)\t0.057300017725114234\n",
      "  (0, 13172)\t0.03701306596891324\n",
      "  (0, 12800)\t0.08381173695376727\n",
      "  (0, 12775)\t0.042588071692877726\n",
      "  (0, 12756)\t0.03644487951808927\n",
      "  (0, 12504)\t0.047093650349899534\n",
      "  (0, 12502)\t0.03136664566252002\n",
      "  (0, 12425)\t0.07579766304265152\n",
      "  (0, 12324)\t0.056168664307386096\n",
      "  (0, 12263)\t0.09262959641488572\n",
      "  (0, 12224)\t0.09262959641488572\n",
      "  (0, 12153)\t0.011190869172231764\n",
      "  (0, 11947)\t0.030862053793468572\n",
      "  (0, 11784)\t0.03165177877275147\n",
      "  (0, 11779)\t0.04081740755061132\n",
      "  (0, 11707)\t0.042026852024637885\n",
      "  (0, 11328)\t0.046088079926399794\n",
      "  (0, 11314)\t0.058828416669491145\n",
      "  (0, 11276)\t0.11434408783521394\n",
      "  :\t:\n",
      "  (0, 2339)\t0.033281005008342\n",
      "  (0, 2315)\t0.0616019187558951\n",
      "  (0, 2302)\t0.3371784221408105\n",
      "  (0, 2131)\t0.049783771439385546\n",
      "  (0, 2081)\t0.07861008993063737\n",
      "  (0, 2077)\t0.04778405500940634\n",
      "  (0, 2071)\t0.09262959641488572\n",
      "  (0, 1947)\t0.05028122529879085\n",
      "  (0, 1788)\t0.05093584869483656\n",
      "  (0, 1771)\t0.08298487686322228\n",
      "  (0, 1742)\t0.04907674461689369\n",
      "  (0, 1616)\t0.33084754820802703\n",
      "  (0, 1431)\t0.15084367589637254\n",
      "  (0, 972)\t0.02652822851927059\n",
      "  (0, 860)\t0.0803351700213948\n",
      "  (0, 703)\t0.0803351700213948\n",
      "  (0, 660)\t0.04635348847299392\n",
      "  (0, 507)\t0.030974752320662456\n",
      "  (0, 442)\t0.07461858729600591\n",
      "  (0, 375)\t0.06436449759187375\n",
      "  (0, 373)\t0.09262959641488572\n",
      "  (0, 372)\t0.07461858729600591\n",
      "  (0, 169)\t0.041434604353682014\n",
      "  (0, 152)\t0.04484797085028183\n",
      "  (0, 127)\t0.06309625110114205\n"
     ]
    }
   ],
   "source": [
    "print(\"email 0:\")\n",
    "print(tfidf[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Utility function\n",
    "\n",
    "Let's put all this loading process into a separate file so that we can reuse it in other experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:43:16.717961Z",
     "start_time": "2018-10-09T14:43:13.235910Z"
    }
   },
   "outputs": [],
   "source": [
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T14:44:08.409209Z",
     "start_time": "2018-10-09T14:44:08.399755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email file: ../data/lingspam_public/bare/part1/spmsga129.txt\n",
      "email is a spam: True\n",
      "Subject: lists and software worldwide\n",
      "\n",
      "order form : all addresses are fresh and cleaned against international remove lists for the best results with the minimum irritation to those who do not wish to recieve unsolicited mail . all discs come with details of web sites for usefull mailing programs and other related products available on the net . many new mailer programs bypass your isp and send mail direct to the recipient so you dont need an expensive \" bulk-friendly \" isp . disc supplied come with a free mailing program , its not the best but will get you started if you dont have one . prices are quoted in uk pound sterling / us dollars and are fully inclusive of postage and packing 1 , 000 , 000 email addresses @ 15 / $ 35 [ ] 2 , 000 , 000 email addresses @ 29 / $ 59 [ ] 3 , 000 , 000 email addresses @ 42 / $ 80 [ ] 4 , 000 , 000 email addresses @ 54 / $ 102 [ ] 5 , 000 , 000 email addresses @ 65 / $ 120 [ ] 6 , 000 , 000 email addresses @ 75 / $ 137 [ ] 7 , 000 , 000 email addresses @ 84 / $ 152 [ ] 8 , 000 , 000 email addresses @ 92 / $ 163 [ ] 9 , 000 , 000 email addresses @ 99 / $ 178 [ ] cd [ ] . zip files by email [ ] email address lists are sent as text documents on cd or by email as . zip files they can be opened with most word processing packages and imported into most data bases and bulk email software . to order from outside the uk send either cheques in your own currency ( the amount should be calcuated from the prices in us dollars not pound sterling , if you are unsure about how to work out the exchange rates a bank will work it out for you if you ask . you just need to ask them what $ xx dollars is in your currency and send that much ) to order tick the number of addresses you wish to purchase and tick the disc type you wish to receive . then fill out the details below and post your order together with payment ( cheques in your own currency or us dollar travellers cheques only please ) made payable to prophoto to : prophoto mail , po box 447 , doman road , camberley , surrey , england gu15 3xd . your email addresses will be dispatched as soon as your payment has cleared ( normally 5 days ) . name : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ address : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ country : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ post / zip code : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ email address : _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ i enclose a cheque to the value of ( enter amount ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ what currency is your payment in ? ( country / currency ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ puchased to date : 1xt\n",
      "\n",
      "Bag of words representation (85 words in dictionary):\n",
      "{'address': array(3), 'amount': array(2), 'ask': array(2), 'available': array(1), 'bank': array(1), 'base': array(1), 'best': array(2), 'box': array(1), 'bulk': array(2), 'bypass': array(1), 'cheque': array(1), 'code': array(1), 'come': array(2), 'country': array(2), 'currency': array(5), 'data': array(1), 'date': array(1), 'day': array(1), 'direct': array(1), 'disc': array(2), 'dollar': array(1), 'dont': array(2), 'either': array(1), 'enclose': array(1), 'enter': array(1), 'exchange': array(1), 'expensive': array(1), 'fill': array(1), 'form': array(1), 'free': array(1), 'fresh': array(1), 'friendly': array(1), 'fully': array(1), 'get': array(1), 'inclusive': array(1), 'international': array(1), 'irritation': array(1), 'made': array(1), 'mail': array(3), 'mailer': array(1), 'many': array(1), 'minimum': array(1), 'much': array(1), 'name': array(1), 'need': array(2), 'net': array(1), 'new': array(1), 'normally': array(1), 'number': array(1), 'one': array(1), 'order': array(4), 'outside': array(1), 'payable': array(1), 'payment': array(3), 'please': array(1), 'po': array(1), 'post': array(2), 'postage': array(1), 'pound': array(2), 'program': array(1), 'purchase': array(1), 'receive': array(1), 'recipient': array(1), 'related': array(1), 'remove': array(1), 'road': array(1), 'send': array(3), 'sent': array(1), 'soon': array(1), 'sterling': array(2), 'subject': array(1), 'surrey': array(1), 'text': array(1), 'tick': array(2), 'together': array(1), 'type': array(1), 'u': array(3), 'unsolicited': array(1), 'unsure': array(1), 'value': array(1), 'web': array(1), 'wish': array(3), 'word': array(1), 'work': array(2), 'zip': array(3)}\n"
     ]
    }
   ],
   "source": [
    "spam_data.print_email(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
